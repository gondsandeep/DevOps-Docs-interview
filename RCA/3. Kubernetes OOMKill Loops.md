# Kubernetes OOMKill Loops

## Incident Details

| Field | Value |
|-------|-------|
| **Severity** | SEV-3 (Moderate) |
| **Service** | User Auth Service |
| **Duration** | Intermittent (2 Weeks) |
| **Status** | Resolved |
| **Owner** | DevOps Team |

---

## 1. Issue Summary

The Authentication Service pods were restarting randomly 10-15 times per day. Users reported intermittent session drops ("I keep getting logged out"). Kubernetes status showed OOMKilled (Out Of Memory).

---

## 2. Business Impact

### User Impact
Degraded User Experience (UX). Frustration due to forced re-logins.

### System Impact
Increased latency during pod startup times (Cold starts).

---

## 3. Root Cause Analysis

**Why were pods crashing?**  
The container memory usage exceeded the Kubernetes limits.

**Why did it exceed the limit?**

### Memory Configuration

| Component | Configured Value |
|-----------|------------------|
| **K8s Limit** | 1Gi (1024 MB) |
| **Java Heap (Xmx)** | 768 MB |

### The Miscalculation
The engineering team assumed 768MB fits inside 1024MB. However, they failed to account for JVM Overhead (Metaspace, Thread Stacks, Garbage Collection, Native Code).

### Actual Memory Usage
- Heap: 768MB
- Non-Heap: ~300MB
- **Total: 1068MB** (exceeds 1024MB limit)

### Root Cause
**Configuration Error.** Misunderstanding of Java memory architecture versus Container Hard Limits.

---

## 4. Resolution & Fix

### Immediate Fix
Increased Kubernetes Memory Limit to 1.5Gi to provide immediate headroom.

### Permanent Fix
- Tuned JVM parameters: Reduced Heap (`-Xmx`) to 512m (approx 50-60% of container limit)
- Enabled JVM flag `-XX:+ExitOnOutOfMemoryError` to allow clean restarts instead of hangs

---

## 5. Action Items

- **[High]** Create a "Golden Signals" dashboard for Memory Usage vs. Limit %
- **[Medium]** Conduct a "Container Sizing" workshop for the backend team

